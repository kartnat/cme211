$ python3 similarity.py ml-100k/u.data similarities.txt
Input MovieLens file: ml-100k/u.data
Output file for similarity data: similarities.txt
Minimum number of common users: 5
Read 100000 lines with total of 1682 movies and 943 users
Computed similarities in 74.87457704544067 seconds

First 10 lines of output:
1 (918,0.91,5)
2 (1056,1.00,5)
3 (1081,0.98,5)
4 (35,0.80,6)
5 (976,0.93,5)
6 (279,0.96,5)
7 (968,1.00,7)
8 (590,0.86,6)
9 (113,0.96,5)
10 (1202,0.97,5)

Chief among my concerns was subscribing to the guidance of the dataset containing 3 movies and 10 users.  Furthermore I tried to capture all three situations that would occur in the similarity computation: one pair of movies for which the denominator in the ratio is zero (all users rated one of the movies the same, for example), one for which there weren't enough (five) users in common to return the ratio, and the last which could be calculated normally.  Then in terms of file format, I just made sure to retain the column ordering and spacing, so the code would generalize to the full dataset.  I did test my final solution on the test data and verified by hand that the parameters returned (ratio, movieid, common users) were correct.  Because my test set was fairly small (~20 rows) this was not hard to do.

I decided to break my code into three functions.  The first one was intended to process the data into a suitable form (dictionary in this case) for the subsequent computations, and return important parameters for the command line log.  The second is where I performed the actual similarity calculation, and the third actually finds the largest ratio, movieid, etc. and then writes the output in a format matching the desired one.  Another option I was considering, if my runtime were too long, was breaking down the last function into one that stored the n(n+1)/2 (rather than n^2) ratios, and another that performed the comparisons/wrote the output.    

***Using first of late days
